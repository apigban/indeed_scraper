#!/usr/bin/env python3.7

import requests
import json
from lxml import html
import dateparser as dp
from datetime import timezone
import time
from get_info import fetch_input

def page_pull():

    arg_list = fetch_input()
    query_url = arg_list[3]

    user_agent = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36'}
    #print(query_url)

    page = requests.get(query_url, headers=user_agent)
    #print(page.content)
    tree = html.fromstring(page.text)
    #print(tree)
    return tree


def date_parser(date_list):
    index = 0
    while index < len(date_list):
        hr_date = dp.parse(date_list[index])
        hr_date = time.mktime(hr_date.timetuple())
        date_list[index] = hr_date
        index += 1
    return date_list

#def db_writer():


#def file_writer(data):
#    with open('out.json') as write_file:
#        json.dump(data, write_file)

#def compile_dict():


def newline_cleaner(job_list):
    index = 0
    while index < len(job_list):
        job_list[index] = job_list[index].strip('\n')
        if job_list[index] == '        ':
            job_list[index] = job_list[index].replace('        ','nothing')
        job_list[index] = job_list[index].strip(' ')
        index += 1
    return job_list

def extractor_jobDetails():

    tree = page_pull()

    jobId_xpath = '//div[@data-tn-component="organicJob"]'

    jobId = tree.xpath(jobId_xpath + '/@id')
    jobLink = tree.xpath(jobId_xpath + '//a[@data-tn-element="jobTitle"]/@href')
    jobTitle = tree.xpath(jobId_xpath + '//a[@data-tn-element="jobTitle"]/@title')
    jobLocation = tree.xpath(jobId_xpath + '//span[@class="location"]/text()')
    jobCompany = newline_cleaner(tree.xpath(jobId_xpath + '//div//span[@class="company"]/text()'))
    jobDate = date_parser(tree.xpath(jobId_xpath + '//div[@class="result-link-bar"]/span[@class="date"]/text()'))
    print(jobCompany)
    jobDict = {
            'jobDetails':{
                'Id':'',
                'Date':'',
                'Link':'',
                'Title':'',
                'Location':'',
                'Company':'',
                }
            }

    index = 0

    while index < len(jobId):
        jobDict['jobDetails']['Id'] = jobId[index]
        jobDict['jobDetails']['Date'] = jobDate[index]
        jobDict['jobDetails']['Link'] = jobLink[index]
        jobDict['jobDetails']['Title'] = jobTitle[index]
        jobDict['jobDetails']['Location'] = jobLocation[index]
        jobDict['jobDetails']['Company'] = jobCompany[index]
        #write to file
        print(jobDict,'\n')
        index += 1


    #for list_element in jobId:
    #    jobDict['jobId'] = list_element
    #    jobDict['jobId']['jobDate'] = str(jobDate[0])
    #jobData = [jobId, jobDate, jobLink, jobTitle, jobLocation, jobCompany]
    #print(jobLink[0])
    #return jobData
    #return jobData

    #def job_toDict(jobAttrib)
    #index = 0

    #while index < len(jobAttrib):
    #    i

    #    hr_date = dp.parse(date_list[index])
    #    date_list[index] = hr_date
    #jobSponsored = has_sponsor(tree, jobId_xpath)
    #jobEasyapply = has_easyapply(tree, jobId_xpath)
    #print(len(jobId), ' jobs available in this page\n')
    #print(jobLink)

    #print(len(jobLink), 'job links found\n')
    #print(len(jobTitle), 'job titles found\n')
    #print(len(jobCompany), 'companies posted\n')
    #print(jobLocation)
    #print(len(jobDate))
    #print(jobDict)


extractor_jobDetails()
#extractor_jobTitle()





#page_pull()
#def search_jobs():
